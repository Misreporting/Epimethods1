{
  "hash": "6626b17403da5a8836c9883625395216",
  "result": {
    "markdown": "# Ecological studies\n\nEcological studies are a class of study designs, not a single design. The distinguishing feature of ecological study has the unit of analysis as an aggregate (e.g., country, county, school, hospital, census block), rather than the individual.\n\nEcological studies include several designs:\n\n-   Group-randomized trial: Usually a randomized ecological study is not referred to as \"ecological,\" but the unit of analysis is at the group level.\n\n-   Cohort, cross-sectional, longitudinal.\n\n### When ecological studies are useful\n\nEcological studies are useful when aggregate information is most readily available, especially for preliminary analysis before proceeding to an individual-level study.\n\n-   Killias study used International Crime Victimization Study (ICVS) data about gun ownership to estimate prevalence of gun ownership, and compared with homicide rate. Homicide is rare, and gun ownership of each victim is unknown.\n\n-   Gun studies have used individual-level data and found that individuals are at greater risk if there's a gun in the home, guns are often used in intimate partner violence, etc., but first step was finding higher rates of violent crime in places with more guns.\n\n-   Other examples where ecological study was the first step: smoking and lung cancer, poverty and pellagra, fluoride and cavities.\n\nEcological studues are also useful when an exposure occurs at the population level: e.g., evaluating the impacts of policies or other geographic-level qualities on disease.\n\n-   Laws in a particular country or state\n\n-   Hospital policy towards premature infants (as in chapter 1).\n\n-   To advocate policies: e.g., Swiss cantons that require gun storage in the central depots might have lower rates of accidental gun injuries.\n\nEcological studues are also useful when exposure has high measurement error, so aggregate measures are more accurate than individual measures.\n\n### Measurements in ecological studies\n\n• Aggregate measures (contextual factors): summary of many individual measures, e.g., % population with syphilis. • Inherently population-level measures (integral measures): e.g., population density, GDP. Multi-level studies use contextual factors and individual-level factors.\n\n• E.g., Individuals within regions. • Multi-level analysis captures the variation both across regions and across individuals.\n\n### How to analyze ecological and multi-level studies\n\nIf the data is continuous, start by plotting the data. Plots are particularly useful because the smaller number of units and lower measurement errors makes it more likely that you'll see a trend. Nearly all data analysis is with a computer.\n\n## Ecological fallacy\n\nInferences about individual behavior cannot be made from aggregate statistics. When people try to make such inferences anyhow, they have committed the ecological fallacy.\n\nExample: Mark Bittman published an oped in the NYT (\"It's the Sugar, Folks\" Feb 27, 2013) with the ecological fallacy, and also inappropriate attribution of causality. http://opinionator.blogs.nytimes.com/2013/02/27/its-the-sugar-folks They had to issue the correction: \"Correction: March 6, 2013 Mark Bittman's column on Thursday incorrectly described findings from a recent epidemiological study of the relationship of sugar consumption to diabetes. The study found that increased sugar in a population's food supply was linked to higher rates of diabetes --- independent of obesity rates --- but stopped short of stating that sugar caused diabetes. It did not find that \"obesity doesn't cause diabetes: sugar does.\" Obesity is, in fact, a major risk factor for Type 2 diabetes, as the study noted.\"\n\n### Cross-level bias\n\nCross-level bias is one reason to avoid committing the ecological fallacy.\n\nThe following plots are an example of cross-level bias, which shows why that extra structure is useful for data analysis, both conceptually and statistically.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mosaic)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'mosaic'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:Matrix':\n\n    mean\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    stat\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n```\n:::\n\n```{.r .cell-code}\nDF = data.frame(exposure = c(rnorm(30, 0.3, 0.05), rnorm(30, 0.6, 0.05), rnorm(30, 0.9, 0.05)), \n           outcome = c(rnorm(30, 0.3, 0.1), rnorm(30, 0.6, 0.1), rnorm(30, 0.9, 0.1)))\nxyplot(outcome ~ exposure, data=DF)\n```\n\n::: {.cell-output-display}\n![](chapt10_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexposure = c(rnorm(30, 0.3, 0.05), rnorm(30, 0.6, 0.05), rnorm(30, 0.9, 0.05), rnorm(30, 1.2, 0.05))\nDF2 = data.frame(exposure,\n           outcome = c(rnorm(30, 0.4, 0.05)-exposure[1:30], rnorm(30, 1.2, 0.05)-exposure[31:60], rnorm(30, 1.9, 0.05)-exposure[61:90], rnorm(30, 2.4, 0.05)-exposure[91:120]))\nxyplot(outcome ~ exposure, data=DF2)\n```\n\n::: {.cell-output-display}\n![](chapt10_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nRich states are more likely to vote Democratic, but within each state, the chances that an individual votes Republican increases with their income.\n\n![](images/red_state_blue_state1.jpg)\n\n![](images/red_state_blue_state2.jpg)\n\n![](images/red_state_blue_state3.png)\n\nSource: Andrew Gelman, Boris Shor, Joseph Bafumi, David Park, \"Rich state, poor state, red state, blue state: What's the matter with Connecticut?\" Popularized in the book Red State, Blue State, Rich State, Poor State: Why Americans Vote the Way They Do. Subsequent analysis in Avi Feller, Andrew Gelman, Boris Shor, \"Red State/Blue State Divisions in the 2012 Presidential Election.\" The Forum 2012; 10(4): 127--131. DOI 10.1515/forum-2013-0014. The text discusses indications that there is a cross-level bias, but often these relationships are found empirically and explanations come after the fact.\n\n### Flaws of ecological studies\n\nEcological fallacy.\n\nGeographical units may not be the most logical division: e.g., metro areas, especially east of the Mississippi, may go across states: DC and NY metro areas both include 3 states. Some exposures are spatial.\nSpatial data analysis might be more useful than considering ecological units.\n\n\n## Application to ecological datasets\nThe Gapminder dataset has data from (up to) 195 countries that can be used to conduct an analysis using a ecological research design. The codebook is available on the course website and lists each variable and the number of countries that each variable is available for. Notice that some variables are only available for a small number of countries, so they are not good candidates for variables to use, unless you are okay having only 20 datapoints.\n\nLoad the mosaic library and the Gapminder dataset. Notice that the variable names in R are all small letters, but the codebook has capital letters in the variable names.\n\nYou can use the provided file or download yourself here:\nhttps://www.gapminder.org/data/ \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mosaic)\nG=read.csv(\"GapMinder.csv\", header=T)\n```\n:::\n\n\nSay that we want to study whether per-capita GDP predicts all-cause child mortality (per 1000 births) before age 5.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistogram(~childdeathall2008, data=G)\n```\n\n::: {.cell-output-display}\n![](chapt10_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxyplot(childdeathall2008 ~ gdp2008, data=G)\n```\n\n::: {.cell-output-display}\n![](chapt10_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nWe notice that childhood death is right-skewed, so we take the log before continuing so that we have a relationship that looks linear.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nG$log.childdeath=log(G$childdeathall2008)\nG$log.gdp=log(G$gdp2008)\nxyplot(log.childdeath ~ gdp2008, data=G, type=c(\"p\", \"r\"))\n```\n\n::: {.cell-output-display}\n![](chapt10_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxyplot(log.childdeath ~ log.gdp, data=G, type=c(\"p\", \"r\"))\n```\n\n::: {.cell-output-display}\n![](chapt10_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nIt looks like we can justify doing a linear regression between these variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1=lm(log.childdeath ~ log.gdp, data=G)\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log.childdeath ~ log.gdp, data = G)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.86287 -0.42523 -0.04222  0.40722  3.05096 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.45197    0.27989   30.20   <2e-16 ***\nlog.gdp     -0.76157    0.03549  -21.46   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7007 on 158 degrees of freedom\n  (35 observations deleted due to missingness)\nMultiple R-squared:  0.7446,\tAdjusted R-squared:  0.7429 \nF-statistic: 460.5 on 1 and 158 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\ncbind(exp(coef(model1)), exp(confint(model1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                               2.5 %       97.5 %\n(Intercept) 4684.2920504 2695.003473 8141.9531482\nlog.gdp        0.4669339    0.435326    0.5008368\n```\n:::\n:::\n\n\n\n|  Parameter | Interpretation | \n|:-----------------|:-----------------|\n| Intercept    | Log child death rate if log(GDP)=0              | \n| p-value for intercept | Is intercept different from 0?            | \n|log.gdp coefficient|Change in log child death rate if log(GDP) increases by one unit|\n|p-value for log.gdp coef|Is coefficient on log.gdp different from 0?|\n|R-squared |Percentage of variation in log child death rate explained by log(GDP)|\n|F-statistic and p-value|Is the overall model significant?\nI.e., does log(GDP) explain log child death rate?|\n\n\nBecause we took the log of both the predictor and outcome variables, the interpretation of these coefficients are different than for regressions where we do not transform the variables.\n\nA $1 increase in per capita log(GDP) predicts 0.76 unit reduction in log child deaths: that is, if GDP increases by 1%, then child deaths decrease by 0.76%.\n\nNow we can think about other variables — are there any variables that may confound the relationship between per-capita GDP and childhood mortality, or is this model sufficient?\n\n\n\n## Exercise\nWork with a small group of classmates to analyze the Gapminder data.\nDownload the Gapminder codebook from the course website on Dropbox. Load the data\n(and the mosaic package) using the above commands. Use the codebook to formuate a research question that you could answer with the Gapminder dataset is neither totally trivial/obvious nor totally arbitrary. The outcome variable should be a continuous variable rather than binary or categorical so that you can use linear regression tools.\n\n1. Identify the research question, the outcome and predictor variables, and potential confounders.\n\n2. Do some basic plots with the outcome and predictor variables to explore the data.\n\n3. Perform a regression with one or more predictors.\n\n4. Prepare a 10 minute informal presentation to discuss your research question, your initial findings, and potential confounders. Make sure to address whether you were surprised by your findings or whether you just found what you expected.\n\n",
    "supporting": [
      "chapt10_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}